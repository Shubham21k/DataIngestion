# Low-Level Design (LLD)

This document dives into component-level details for the **Metastore Service**, **Airflow DAGs**, **Metrics Service**, and **Ingestion Framework** (Spark jobs) that make up the ingestion platform.

---

## 1. Metastore Service

### 1.1 Technology Stack
| Layer | Choice |
|-------|--------|
| Language | Python 3.11 |
| Framework | FastAPI + Pydantic |
| Persistence | SQLite (local demo) / PostgreSQL (prod) |
| Migrations | Alembic |
| Glue Sync | `boto3` Glue SDK |

### 1.2 DB Schema (ERD)
```
+-----------------+      +---------------------+
|  datasets       |      |  schema_versions    |
+-----------------+      +---------------------+
| id (PK)         |<--+  | id (PK)            |
| name            |   |  | dataset_id (FK)    |
| kafka_topic     |   +--| version            |
| mode            |      | ddl_sql            |
| pk_fields       |      | created_at         |
| partition_keys  |      +---------------------+
| transform_jars  |
| created_at      |
| updated_at      |
+-----------------+

+------------------+
| ddl_history      |
+------------------+
| id (PK)          |
| dataset_id (FK)  |
| ddl_sql          |
| glue_synced (Y/N)|
| created_at       |
+------------------+
```

### 1.3 REST API
| Verb | Path | Purpose |
|------|------|---------|
| GET  | `/datasets` | List datasets. |
| POST | `/datasets` | Create dataset. |
| GET  | `/datasets/{id}` | Retrieve config JSON. |
| PATCH| `/datasets/{id}` | Update mode / transforms. |
| POST | `/datasets/{id}/ddl` | Submit `ADD COLUMN / DROP COLUMN` SQL; row inserted into `ddl_history`; Glue catalog updated asynchronously. |

All responses are JSON and conform to an OpenAPI spec auto-generated by FastAPI.

### 1.4 Schema Versioning & Evolution Flow
The `schema_versions` table stores an immutable history of table schemas, enabling reproducible backfills.

| Column | Type | Notes |
|--------|------|-------|
| `id` | BIGINT PK | Surrogate key. |
| `dataset_id` | FK | Links to `datasets`. |
| `version` | INT | Monotonic per dataset. |
| `schema_json` | TEXT | Spark `StructType` JSON. |
| `created_at` | TIMESTAMP | Insertion time. |
| `status` | ENUM(ACTIVE,PENDING,OBSOLETE) | Current applicability. |

#### Evolution lifecycle
1. **Detection** (Phase-2 job)
   * Unknown column → emits `ADD COLUMN` request (see §4.5) which creates a **PENDING** row in `schema_versions` with the merged schema.
2. **Glue update** (Celery worker)
   * After Glue `ALTER TABLE` succeeds, row status becomes **ACTIVE** and `version` increments.
   * Previous ACTIVE row flips to **OBSOLETE**.
3. **Job refresh**
   * Next micro-batch broadcasts `activeSchemaVersionId`; when mismatch detected, driver fetches new schema JSON and caches it.
4. **Drop column**
   * Admin API call marks column inactive and produces a merged schema without that field; same PENDING→ACTIVE promotion applies.
5. **Backfill**
   * Backfill DAG can pin `--schema-version=N` to replay historical layouts exactly.

#### Handling **Breaking** Schema Changes
Breaking changes include type narrowing (e.g., BIGINT → INT), primary-key change, or column rename that would invalidate readers.
1. Phase-2 job detects incompatible change via `SchemaInspector.isBreaking(newType, oldType)`.
2. Driver **fails the micro-batch** and publishes metric `breaking_schema_change=1`; Airflow marks task failed.
3. Metastore sets the new schema row as `status = BLOCKED` and records the reason.
4. Operator must either:
   * Provide a **custom cast/transform JAR** to make data compatible, then resubmit schema as additive change, **or**
   * Create a **new dataset**/table and backfill from raw dumps.
5. Once remedied, admin updates the row to PENDING to re-enter normal lifecycle.

This fail-fast strategy prevents silent corruption while giving operators a manual recovery path.

### 1.5 Glue Sync Flow
1. API stores DDL in `ddl_history` with `glue_synced = 'N'`.
2. Background Celery worker polls unsynced rows every 30 s.
3. Worker runs `boto3.glue.update_table` or `create_table` as required.
4. On success, `glue_synced = 'Y'`; on failure, row stays pending and surfaced in `/health`.

### 1.6 Security & Observability
* HTTP Basic Auth (demo) → replace with IAM / OIDC in prod.
* `uvicorn` access logs + structured JSON logs.
* Prometheus instrumentation via `prometheus-fastapi-instrumentator`.

---

## 2. Airflow DAGs

### 2.1 DAG Structure
```
├─ ingestion_phase1_<dataset>
│   ├─ start → spark_raw_dump → success
│
└─ ingestion_phase2_<dataset>
    ├─ start → spark_ingest_lakehouse → metrics_push → success
```

* **spark_raw_dump** – KubernetesPodOperator / SparkSubmitOperator launching Phase-1 JAR.
* **spark_ingest_lakehouse** – launches Phase-2 JAR.
* DAGs are generated dynamically from Metastore at scheduler start-up via a custom `metastore_plugin.py`.
* Backfill DAG reuses `spark_ingest_lakehouse` with a date range parameter.

### 2.2 Failure Handling
* Operator retries = 3, exponential back-off.
* If Phase-2 fails, downstream **metrics_push** is skipped; PagerDuty alert via SlackWebhookOperator.
* Manual trigger UI for backfill date windows.

---

## 3. Metrics Service

### 3.1 Implementation
* Lightweight Flask app started inside the Spark driver JVM via `py4j` gateway (adds negligible overhead).
* Exposes `/metrics` returning:
```jsonc
{
  "pipeline": "orders_phase2",
  "timestamp": "2025-06-24T00:00:00Z",
  "records": 120000,
  "lag_seconds": 5,
  "duration_ms": 3800,
  "error_count": 0
}
```
* Spark emits events to the Flask queue every micro-batch; last snapshot served.

### 3.2 Deployment & Port
* Driver pod exposes port `8090`; Airflow **metrics_push** task can scrape and forward to external systems later.

---

## 4. Ingestion Framework (Spark)

### 4.1 Packaging
| Item | Details |
|------|---------|
| Language | Scala 2.12 |
| Build | sbt-assembly JAR (fat JAR) |
| Runtime | Spark 3.5 Structured Streaming |

### 4.2 Phase-1 Logic
```scala
val df = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", env.kafka)
  .option("subscribe", topic)
  .option("startingOffsets", "latest")
  .load()

val parsed = df.selectExpr("CAST(value AS STRING) as json", "topic", "partition", "offset", "timestamp")

parsed.writeStream
  .format("json")
  .option("path", s"s3://raw/$topic/date=${today}")
  .option("checkpointLocation", s"/chk/$topic/phase1")
  .start()
```

### 4.3 Phase-2 Logic
```scala
val raw = spark.readStream
  .schema(latestSchemaFromMetastore())
  .json(rawPath)

val transformed = TransformRunner.run(raw, jarsFromMetastore())

val writer = if (mode == "append") transformed.writeStream.format("parquet")
             else transformed.writeStream.format("hudi")

writer
  .option("path", lakehousePath)
  .option("checkpointLocation", s"/chk/$topic/phase2")
  .start()
```

### 4.4 Transform Plug-in SPI
```scala
trait Transformer extends Serializable {
  def transform(ds: Dataset[Row]): Dataset[Row]
}
```
* JARs are loaded via URLClassLoader; list obtained from Metastore.

### 4.5 Schema Evolution Handling
1. On every micro-batch the driver reads `activeSchema` (version id cached in a broadcast variable).
2. **Add column (new field detected)**
   * Ingest job infers Spark data type and immediately issues a **DML-style Glue update**:
     ```scala
     spark.sql(s"ALTER TABLE ${db}.${table} ADD COLUMN ${col} ${sparkType}")
     ```
     or via Glue SDK:
     ```python
     glue.update_table(DatabaseName=db, TableInput=updatedTable)
     ```
   * The same command is also persisted in Metastore `ddl_history` as reference.
   * Current batch writes the column; readers with old schema will just ignore it.
3. **Drop column request**
   * We never physically drop in Glue; instead Metastore marks column as *inactive*.
   * Phase-2 projects only **active columns**, but keeps a placeholder `lit(null).alias(col)` so downstream schema remains fixed and future rows carry NULLs.
   * Historical data in raw dump/Lakehouse still has the old column—no migration needed.

### 4.6 Checkpointing & Exactly-Once
We will use **Spark’s native checkpoint mechanism** and simply point it to an `s3a://` path.

* **Directory**: `s3a://checkpoints/<pipeline>/<phase>`
* Stores offsets, commit logs and (when needed) state-store files—guaranteeing exactly-once without extra code.
* Works for both phases; `spark.sql.streaming.noDataProgressEventInterval=10s` keeps metrics flowing even when idle.
* S3A optimisation flags set via Spark conf:
  ```
  spark.hadoop.fs.s3a.fast.upload=true
  spark.hadoop.fs.s3a.directory.marker.policy=keep
  ```
This avoids the complexity of a custom offset store while staying compatible with future stateful operations.

---

## 5. Operational Optimizations & Edge-Cases

### Metastore Concurrency
* **Unique index** on `datasets.name` avoids duplicate entries.
* **Optimistic locking column** `version` (incremented on each update) prevents lost updates when multiple admins post DDL simultaneously.

### Airflow Auto-Scaling
* Spark CPU / memory parameters (`executorMemory`, `executorCores`, `maxExecutors`) are stored in Metastore and templated into DAG operators so high-throughput topics launch larger clusters automatically.

### Ingestion Framework Tweaks
* **Phase-1**
  * `failOnDataLoss=false` to continue past Kafka retention gaps; emits metric `data_loss_detected`.
  * `maxOffsetsPerTrigger` pulled from Metastore to throttle per-batch volume.
* **Phase-2**
  * **Idempotent Hudi writes**: embed `batchId` in commit metadata; skip if commit already exists.
  * **Isolated ClassLoaders** for each transform JAR to avoid shading conflicts.
* **Schema evolution edge cases**
  * **Type widening**: if incoming type can safely widen (e.g., INT → BIGINT) cast to target; otherwise raise alert.
  * **Nested structures**: not supported in P1; raw JSON kept, Phase-2 flattens first level only.

### Checkpointing Hardening
* S3 eventual consistency mitigated via `spark.hadoop.fs.s3a.committer.name=directory`.
* Weekly housekeeping job truncates `_spark_metadata` to the last *N* commits (configurable) to control checkpoint growth.

### Security / IAM
* Spark driver/executor pods assume an **IRSA IAM role** granting S3 & Glue access.
* Metastore uses the same role via AWS SDK credentials on the server side; no hard-coded keys.

---

## 6. Glossary (quick reference)
| Term | Meaning |
|------|---------|
| **Phase-1** | Kafka → raw JSON on S3. |
| **Phase-2** | Raw JSON → transformed Lakehouse data. |
| **DDL/DML Sync** | Metastore updating Glue tables to reflect schema evolution or mode changes. |
