# Low-Level Design (LLD)

This document provides detailed component-level specifications for the **Java Metastore Service**, **Airflow DAGs**, **Spark Ingestion Framework**, and **Schema Evolution** that comprise the production-ready ingestion platform.

---

## 1. Metastore Service

### 1.1 Technology Stack
| Layer | Choice |
|-------|--------|
| Language | Java 17 |
| Framework | Spring Boot 3.2 + Spring Data JPA |
| Persistence | PostgreSQL with HikariCP connection pooling |
| Migrations | Hibernate DDL auto-update |
| Validation | Jakarta Bean Validation |
| Monitoring | Spring Boot Actuator |

### 1.2 Database Schema (JPA Entities)
```
+------------------+      +----------------------+
|  datasets        |      |  schema_versions     |
+------------------+      +----------------------+
| id (PK)          |<--+  | id (PK)             |
| name (UNIQUE)    |   |  | dataset_id (FK)     |
| kafka_topic      |   +--| version             |
| mode             |      | schema_json (TEXT)  |
| pk_fields (JSON) |      | status (ENUM)       |
| partition_keys   |      | created_at          |
| transform_jars   |      +----------------------+
| created_at       |
| updated_at       |
+------------------+

+------------------+
| ddl_history      |
+------------------+
| id (PK)          |
| dataset_id (FK)  |
| ddl_sql (TEXT)   |
| glue_synced      |
| created_at       |
+------------------+
```

**Schema Status Enum**: `ACTIVE`, `PENDING`, `OBSOLETE`, `BLOCKED`

### 1.3 Service Architecture

**Layered Architecture**:
- **Controller Layer**: REST endpoints with validation
- **Service Layer**: Business logic and transaction management
- **Repository Layer**: JPA data access with custom queries
- **Entity Layer**: JPA entities with relationships

**Key Features**:
- **Transaction Management**: `@Transactional` for ACID compliance
- **Validation**: Jakarta Bean Validation on DTOs
- **Exception Handling**: Global exception handler with proper HTTP status codes
- **Health Checks**: Spring Boot Actuator endpoints
- **Connection Pooling**: HikariCP for database connections

### 1.4 REST API
| Verb | Path | Purpose |
|------|------|---------|
| GET  | `/` | Service info and health |
| GET  | `/health` | Health check endpoint |
| GET  | `/datasets` | List all datasets |
| POST | `/datasets` | Create new dataset |
| GET  | `/datasets/{id}` | Get dataset by ID or name |
| PATCH | `/datasets/{id}` | Update dataset configuration |
| GET  | `/datasets/{id}/schema/active` | Get active schema version |
| GET  | `/datasets/{id}/schema/versions` | List all schema versions |
| POST | `/datasets/{id}/schema/evolve` | Handle schema evolution |
| PATCH| `/datasets/{id}` | Update mode / transforms. |
| POST | `/datasets/{id}/ddl` | Submit `ADD COLUMN / DROP COLUMN` SQL; row inserted into `ddl_history`; Glue catalog updated asynchronously. |

All responses are JSON and conform to an OpenAPI spec auto-generated by FastAPI.

### 1.4 Schema Versioning & Evolution Flow
The `schema_versions` table stores an immutable history of table schemas, enabling reproducible backfills.

| Column | Type | Notes |
|--------|------|-------|
| `id` | BIGINT PK | Surrogate key. |
| `dataset_id` | FK | Links to `datasets`. |
| `version` | INT | Monotonic per dataset. |
| `schema_json` | TEXT | Spark `StructType` JSON. |
| `created_at` | TIMESTAMP | Insertion time. |
| `status` | ENUM(ACTIVE,PENDING,OBSOLETE,BLOCKED) | Current applicability. |

#### Evolution lifecycle
1. **Detection** (Phase-2 job)
   * Unknown column → emits `ADD COLUMN` request (see §4.5) which creates a **PENDING** row in `schema_versions` with the merged schema.
2. **Glue update** (Celery worker)
   * After Glue `ALTER TABLE` succeeds, row status becomes **ACTIVE** and `version` increments.
   * Previous ACTIVE row flips to **OBSOLETE**.
3. **Job refresh**
   * Next micro-batch broadcasts `activeSchemaVersionId`; when mismatch detected, driver fetches new schema JSON and caches it.
4. **Drop column**
   * Admin API call marks column inactive and produces a merged schema without that field; same PENDING→ACTIVE promotion applies.
5. **Backfill**
   * Backfill DAG can pin `--schema-version=N` to replay historical layouts exactly.

#### Handling **Breaking** Schema Changes
Breaking changes include type narrowing (e.g., BIGINT → INT), primary-key change, or column rename that would invalidate readers.
1. Phase-2 job detects incompatible change via `SchemaInspector.isBreaking(newType, oldType)`.
2. Driver **fails the micro-batch** and publishes metric `breaking_schema_change=1`; Airflow marks task failed.
3. Metastore sets the new schema row as `status = BLOCKED` and records the reason.
4. Operator must either:
   * Provide a **custom cast/transform JAR** to make data compatible, then resubmit schema as additive change, **or**
   * Create a **new dataset**/table and backfill from raw dumps.
5. Once remedied, admin updates the row to PENDING to re-enter normal lifecycle.

This fail-fast strategy prevents silent corruption while giving operators a manual recovery path.

### 1.5 Glue Sync Flow
1. API stores DDL in `ddl_history` with `glue_synced = 'N'`.
2. Background Celery worker polls unsynced rows every 30 s.
3. Worker runs `boto3.glue.update_table` or `create_table` as required.
4. On success, `glue_synced = 'Y'`; on failure, row stays pending and surfaced in `/health`.

### 1.6 Security & Observability
* HTTP Basic Auth (demo) → replace with IAM / OIDC in prod.
* `uvicorn` access logs + structured JSON logs.
* Prometheus instrumentation via `prometheus-fastapi-instrumentator`.

---

## 2. Airflow DAGs

### 2.1 DAG Structure
```
├─ ingestion_phase1_<dataset>
│   ├─ start → spark_raw_dump → success
│
└─ ingestion_phase2_<dataset>
    ├─ start → spark_ingest_lakehouse → metrics_push → success
```

* **spark_raw_dump** – KubernetesPodOperator / SparkSubmitOperator launching Phase-1 JAR.
* **spark_ingest_lakehouse** – launches Phase-2 JAR.
* DAGs are generated dynamically from Metastore at scheduler start-up via a custom `metastore_plugin.py`.
* Backfill DAG reuses `spark_ingest_lakehouse` with a date range parameter.

### 2.2 Failure Handling
* Operator retries = 3, exponential back-off.
* If Phase-2 fails, downstream **metrics_push** is skipped; PagerDuty alert via SlackWebhookOperator.
* Manual trigger UI for backfill date windows.

---

## 3. Metrics Service

### 3.1 Implementation
* Lightweight Flask app started inside the Spark driver JVM via `py4j` gateway (adds negligible overhead).
* Exposes `/metrics` returning:
```jsonc
{
  "pipeline": "orders_phase2",
  "timestamp": "2025-06-24T00:00:00Z",
  "records": 120000,
  "lag_seconds": 5,
  "duration_ms": 3800,
  "error_count": 0
}
```
* Spark emits events to the Flask queue every micro-batch; last snapshot served.

### 3.2 Deployment & Port
* Driver pod exposes port `8090`; Airflow **metrics_push** task can scrape and forward to external systems later.

---

## 4. Ingestion Framework (Spark)

### 4.1 Technology Stack
| Layer | Choice |
|-------|--------|
| Language | Scala 2.12 |
| Framework | Spark 3.4.1 (Structured Streaming) |
| Build Tool | Maven with Scala plugin and Shade plugin |
| Serialization | Kryo serializer |
| Checkpointing | S3A with exactly-once semantics |
| Lakehouse | Apache Hudi (upsert) / Parquet (append) |
| Logging | Log4j with structured logging |
| Configuration | Centralized config management |

### 4.2 Modular Core Framework

**Core Components**:
- **IngestionConfig**: Centralized configuration parsing with scopt
- **MetastoreClient**: HTTP client for Metastore API integration
- **SchemaEvolution**: Schema comparison and evolution logic
- **TransformerLoader**: Dynamic JAR loading from S3
- **LoggingUtils**: Consistent logging patterns
- **SparkUtils**: Common Spark configurations and utilities

### 4.3 Phase-1 Implementation
```scala
val config = IngestionConfig.parsePhase1Args(cmdArgs)
val logger = LoggingUtils.setupJobLogging(getClass.getName)

val spark = SparkUtils.createOptimizedSparkSession(
  s"Phase1-${config.topic}", 
  SparkUtils.getS3ALocalStackConfig()
)

val kafkaOptions = SparkUtils.getKafkaSourceOptions(
  config.kafkaBootstrapServers, config.topic, config.startingOffsets
)
val kafkaStream = spark.readStream.format("kafka")
kafkaOptions.foreach { case (key, value) => kafkaStream.option(key, value) }

val structuredStream = kafkaStream.load().selectExpr(
  "CAST(value AS STRING) AS json",
  "struct(topic, partition, offset, timestamp, CAST(key AS STRING) as key) AS _meta"
)

structuredStream.writeStream
  .format("json")
  .option("path", config.rawPath)
  .option("checkpointLocation", config.checkpoint)
  .trigger(SparkUtils.Triggers.FAST_PROCESSING)
  .start()
```

### 4.4 Phase-2 Implementation
```scala
// Configuration and setup
val config = IngestionConfig.parsePhase2Args(cmdArgs)
val spark = SparkUtils.createOptimizedSparkSession(s"Phase2-${config.dataset}", s3Config)
val metastoreClient = new MetastoreClient(config.metastoreUrl)

// Fetch dataset configuration and transformers
val datasetConfig = metastoreClient.getDatasetConfig(config.dataset)
val transformers = TransformerLoader.loadTransformers(datasetConfig.transformJars)

// Read raw JSON data
val dataStream = spark.readStream.format("json").load(config.rawPath)
val currentSchema = dataStream.schema

// Schema evolution handling
val activeSchemaOpt = metastoreClient.getActiveSchemaVersion(datasetConfig.id)
activeSchemaOpt match {
  case Some(activeSchemaJson) =>
    SchemaEvolution.parseSchemaJson(activeSchemaJson) match {
      case Success(activeSchema) =>
        val comparison = SchemaEvolution.compareSchemas(activeSchema, currentSchema)
        if (!SchemaEvolution.handleSchemaEvolution(comparison, datasetConfig.id, metastoreClient)) {
          throw new RuntimeException("Breaking schema change detected - job terminated")
        }
        // Update schema for non-breaking changes
        if (comparison.changeType == SchemaEvolution.NonBreaking) {
          metastoreClient.updateActiveSchema(datasetConfig.id, currentSchema.json, "NON_BREAKING")
        }
      case Failure(e) => 
        metastoreClient.updateActiveSchema(datasetConfig.id, currentSchema.json, "INITIAL")
    }
  case None =>
    // Capture initial schema
    metastoreClient.updateActiveSchema(datasetConfig.id, currentSchema.json, "INITIAL")
}

// Apply transformers sequentially
val transformedStream = transformers.foldLeft(dataStream) { (ds, transformer) =>
  transformer.transform(ds)
}

// Configure writer based on mode
val writer = datasetConfig.mode match {
  case "append" =>
    transformedStream.writeStream.format("parquet").outputMode("append")
  case "upsert" =>
    val primaryKey = datasetConfig.pkFields.headOption.getOrElse("id")
    val hudiOptions = SparkUtils.getHudiUpsertOptions(config.dataset, primaryKey)
    val hudiWriter = transformedStream.writeStream.format("hudi")
    hudiOptions.foreach { case (key, value) => hudiWriter.option(key, value) }
    hudiWriter.outputMode("append")
}

writer
  .option("path", config.lakePath)
  .option("checkpointLocation", config.checkpoint)
  .trigger(SparkUtils.Triggers.NORMAL_PROCESSING)
  .start()
```

### 4.5 Dynamic Transformer Loading
```scala
// Base transformer interface
trait Transformer extends Serializable {
  def transform(ds: Dataset[Row]): Dataset[Row]
}

// Dynamic loading implementation
object TransformerLoader {
  def loadTransformers(jarUrls: List[String]): List[Transformer] = {
    jarUrls.flatMap { jarUrl =>
      val classLoader = new URLClassLoader(Array(new URL(jarUrl)), getClass.getClassLoader)
      
      // Multiple strategies for finding transformer classes
      val transformerClasses = findTransformerClasses(classLoader, jarUrl)
      
      transformerClasses.map { className =>
        val transformerClass = classLoader.loadClass(className)
        transformerClass.getDeclaredConstructor().newInstance().asInstanceOf[Transformer]
      }
    }
  }
  
  private def findTransformerClasses(classLoader: URLClassLoader, jarUrl: String): List[String] = {
    // Scan JAR for Transformer implementations
    // Multiple naming patterns supported
  }
}
```

**Features**:
- **URLClassLoader**: Dynamic JAR loading from S3
- **Multiple Discovery**: Class name patterns and JAR scanning
- **Validation**: Type checking and error handling
- **Logging**: Comprehensive loading and execution logs

### 4.5 Schema Evolution Handling
1. On every micro-batch the driver reads `activeSchema` (version id cached in a broadcast variable).
2. **Add column (new field detected)**
   * Ingest job infers Spark data type and immediately issues a **DML-style Glue update**:
{{ ... }}
```scala
  .option("checkpointLocation", config.checkpoint)
  .trigger(SparkUtils.Triggers.NORMAL_PROCESSING)
  .start()
```
     spark.sql(s"ALTER TABLE ${db}.${table} ADD COLUMN ${col} ${sparkType}")
     ```
     or via Glue SDK:
     ```python
     glue.update_table(DatabaseName=db, TableInput=updatedTable)
     ```
   * The same command is also persisted in Metastore `ddl_history` as reference.
   * Current batch writes the column; readers with old schema will just ignore it.
3. **Drop column request**
   * We never physically drop in Glue; instead Metastore marks column as *inactive*.
   * Phase-2 projects only **active columns**, but keeps a placeholder `lit(null).alias(col)` so downstream schema remains fixed and future rows carry NULLs.
   * Historical data in raw dump/Lakehouse still has the old column—no migration needed.

### 4.6 Checkpointing & Exactly-Once
We will use **Spark’s native checkpoint mechanism** and simply point it to an `s3a://` path.

* **Directory**: `s3a://checkpoints/<pipeline>/<phase>`
* Stores offsets, commit logs and (when needed) state-store files—guaranteeing exactly-once without extra code.
* Works for both phases; `spark.sql.streaming.noDataProgressEventInterval=10s` keeps metrics flowing even when idle.
* S3A optimisation flags set via Spark conf:
  ```
  spark.hadoop.fs.s3a.fast.upload=true
  spark.hadoop.fs.s3a.directory.marker.policy=keep
  ```
This avoids the complexity of a custom offset store while staying compatible with future stateful operations.

---

## 5. Operational Optimizations & Edge-Cases

### Metastore Concurrency
* **Unique index** on `datasets.name` avoids duplicate entries.
* **Optimistic locking column** `version` (incremented on each update) prevents lost updates when multiple admins post DDL simultaneously.

### Airflow Auto-Scaling
* Spark CPU / memory parameters (`executorMemory`, `executorCores`, `maxExecutors`) are stored in Metastore and templated into DAG operators so high-throughput topics launch larger clusters automatically.

### Ingestion Framework Tweaks
* **Phase-1**
  * `failOnDataLoss=false` to continue past Kafka retention gaps; emits metric `data_loss_detected`.
  * `maxOffsetsPerTrigger` pulled from Metastore to throttle per-batch volume.
* **Phase-2**
  * **Idempotent Hudi writes**: embed `batchId` in commit metadata; skip if commit already exists.
  * **Isolated ClassLoaders** for each transform JAR to avoid shading conflicts.
* **Schema evolution edge cases**
  * **Type widening**: if incoming type can safely widen (e.g., INT → BIGINT) cast to target; otherwise raise alert.
  * **Nested structures**: not supported in P1; raw JSON kept, Phase-2 flattens first level only.

### Checkpointing Hardening
* S3 eventual consistency mitigated via `spark.hadoop.fs.s3a.committer.name=directory`.
* Weekly housekeeping job truncates `_spark_metadata` to the last *N* commits (configurable) to control checkpoint growth.

### Security / IAM
* Spark driver/executor pods assume an **IRSA IAM role** granting S3 & Glue access.
* Metastore uses the same role via AWS SDK credentials on the server side; no hard-coded keys.

---

## 6. Glossary (quick reference)
| Term | Meaning |
|------|---------|
| **Phase-1** | Kafka → raw JSON on S3. |
| **Phase-2** | Raw JSON → transformed Lakehouse data. |
| **DDL/DML Sync** | Metastore updating Glue tables to reflect schema evolution or mode changes. |
